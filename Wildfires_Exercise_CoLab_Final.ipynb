{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acse-ej321/outreachevent/blob/main/Wildfires_Exercise_CoLab_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moXRV2lHb0iF"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWumo5HM8mtH"
      },
      "source": [
        "## **Welcome to Code The Planet!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48TOhp6bptJe"
      },
      "source": [
        "## **Objectives: What we hope you get out of today**\n",
        "\n",
        "Today you will have the opportunity to apply your coding skills to a real world dataset much like a professional Data Scientist and like many of the PhD students here today do addressing their own reseasrch areas in the Earth Sciences.\n",
        "\n",
        "Many times you will start with an interesting question or problem. Then, formulate an idea of how you will go about tackling the problem sometimes guided by what real world data you can find.\n",
        "\n",
        "\n",
        "We will be working through an example today of how the coding, maths, deductive reasoning, and teamwork skills you have been learning are applied to a real workd scenario a Data Scientist or PhD student might encounter.\n",
        "\n",
        "\n",
        "The PhD students and staff here come from wonderfully diverse backgrounds and experiences. What we all have in common is we were once in your shoes in school studying for exams and wondering occasionally:\n",
        "- \"when will I ever need this again?\"\n",
        "- \"what am I going to study at university?\"\n",
        "- \"how will this workshop help me pass my exam?\"\n",
        "\n",
        "\n",
        "Objectives\n",
        "- Have fun with coding!\n",
        "- Collaborate and share ideas with others\n",
        "- Approach the exercises with curiousity\n",
        "- Ask lots of questions\n",
        "- Be challenged - if you need more difficulty, just ask!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Schedule for the Day**\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?id=1K5bC66N1BgABkxJ-E6WL7fd1Do22LVEK\" width=\"800\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "IHSMqiuJ-bRn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VUP_riablzu"
      },
      "source": [
        "## **Ice Breaker #1**\n",
        "\n",
        "The first activity for the day will be dividing ourselves into teams for the activity. This will also be the first opportunity for the first 3 teams to win some prizes. So please pay attention to the directions!\n",
        "\n",
        "\n",
        "**Task**\n",
        "\n",
        "We will hand you each a question. There are 5-7 other students who will have the same question.\n",
        "\n",
        "Your task:\n",
        "- find the other students with the same question.This will be your group.\n",
        "- Once you have formed the group, work together to answer the question.\n",
        "- when you have the answer, raise your hands to check the answer.\n",
        "- the first 3 teams to get the right answer will win prizes!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ice Breaker #2**\n",
        "\n",
        "In your groups discuss your interests and find three things you have in common. For example:\n",
        "- favourite film\n",
        "- hobbies\n",
        "- least favourite subject at school\n",
        "- favourite colour"
      ],
      "metadata": {
        "id": "TrUkD7ZWSaCu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKpKh-oiBRTt"
      },
      "source": [
        "# **Wildfires Exercise**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Computer Setup**\n",
        "\n",
        "**Please follow the steps below in order to get logged into the university computer and get a copy ofthe activty workbook**\n",
        " we will be using for the rest of the day. If you encounter any issues, please raise your hand and one of the volunteers will be right with you to help troubleshoot. We will do our best to get you all up and running as fast as possible.\n",
        "\n",
        "### **Instructions**\n",
        "\n",
        "**Login to University Computer**\n",
        "\n",
        "1. Press Ctrl+Alt+Delete to unlock the computer\n",
        "\n",
        "2. Follow the login prompt and enter the 'GuestID' as your User name and 'GuestPassword' as your Password\n",
        "\n",
        "3. If there are any random prompts which appear, click 'No', or 'Cancel' on them\n",
        "\n",
        "**Download the Activity Notebook**\n",
        "\n",
        "1. From either Bing or Chrome Browser windows, navigate to the following path: https://github.com/acse-ej321/outreachevent\n",
        "\n",
        "2. Click on the file which is called \"\"Wildfires_Exercise_CoLab_Final.ipynb\"\n",
        "\n",
        "3. There is a button in the far, right hand side with an arrow pointing down to 'Download raw file'. Click on this and download the file to your Downloads folder (this should be the default destination)\n",
        "\n",
        "**Option A: Login to Google CoLab**\n",
        "\n",
        "You will need a Google Account to use the Google CoLab version of the Workbook.\n",
        " - If you have a @gmail account already, that is all you need to use CoLab.\n",
        " - If you don't have a Google Account and would like to use Google CoLab, please raise your hand and we will walk you through setting up a new or temporary Google Account.\n",
        " - If you don't wish to register for Google - please skip to Option B instructions below for accessing the Notebook.\n",
        "\n",
        "1. Open up Google Chrome from the Desktop icon\n",
        "\n",
        "2. Click on this [link](https://colab.research.google.com/?utm_source=scs-index) to take you to Google colab\n",
        "\n",
        "3. Chick on 'Sign in' in the upper right corner and enter your Google Account login username and password.\n",
        "\n",
        "4. Once you are logged into Google Colab, click 'File' in the upper left corner and choose 'Open Notebook'\n",
        "\n",
        "5. From the tabs at the top, click on \"Upload\" and navigate to the 'Downloads' folder and upload the \"Wildfires_Exercise_CoLab_Final.ipynb\". Feel free to rename it by clicking 'File' then 'Rename'.\n",
        "\n",
        "\n",
        "**Option B: Login to Jupyter Notebook**\n",
        "\n",
        "1. When you login to the computer, a browser will have popped up with SoftwareHub splash page.\n",
        "\n",
        "2. In the search bar, type \"Anaconda\", and select \"Anaconda3\" from the list of options. This will launch the Anaconda package - you don't have to do anything with this, just let it run in the background.\n",
        "\n",
        "3. Go to the start menu 'Search' bar and type, \"PowerShell\", and select \"Windows PowerShell\" from the menu options.\n",
        "\n",
        "4. In the Command Line Interface (CLI) prompt which opens type: \"  c:\\temp\\jup1.ps1\" and hit Enter. This will run some code to fix a bug we found.\n",
        "\n",
        "5. In the start menu 'Search bar now type \"Jupyter\", and select \"Jupyter Notebook (Anaconda3)\" from the menu.\n",
        "\n",
        "6. This will open a browser window with a menu. Click on 'Downloads' and then the Notebook to launch the exercise. Feel free to rename it by clicking 'File' then 'Rename'.\n",
        "\n",
        "7. For the entire notebook to work you will need to click the \"Not Trusted\" button in the upper right hand corner. In the pop-up window, select \"Trust\". This is important for Part 2 of the exercise.\n",
        "\n"
      ],
      "metadata": {
        "id": "c8mxD1HukhJV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_KTBtJ7biMw"
      },
      "source": [
        "## **Introduction**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fpzEBKsA0Iq"
      },
      "source": [
        "### **Activity Overview: Roadmap**\n",
        "\n",
        "As mentioned above, you will have the opportunity to **apply your skills to a real world dataset much like a professional Data Scientist or Researcher** would in their own research area in the Earth Sciences. One of those areas being the theme of today's activity: **Wildfires**.\n",
        "\n",
        "We will be focusing on how our dataset can help us both better understand wildfires as well as potentially make valuable predictions about them.\n",
        "\n",
        "The exercise is organized following a common data analytics workflow:\n",
        "\n",
        "- #### **Part 0: Data Preparation** - acquiring and cleaning the data\n",
        "- #### **Part 1: Data Exploration** - investigating the data usefulness and limitations\n",
        "- #### **Part 2: Data Visualisation** - looking for meaningful trends and communicating observations\n",
        "- #### **Part 3: Machine Learning** - using the data to make meaningful predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUoET1U7uGRO"
      },
      "source": [
        "### **Navigating the Notebook**\n",
        "\n",
        "Today we will be using Coding Notebooks, either Google CoLab or Jupyter Notebook, which have become very popular tools for working with code collaboratively. Some basics to help you navigate this notebook:\n",
        "\n",
        " - There are 2 types of 'cells' - text and code. If you look at the top of your screen you can see two buttons `+ Code` and `+ Text`. These are used for creating new cells. We have provided you with all the text and code cells you will need to do the exercise, but feel free to add either to take notes, add your own code, or just play around.\n",
        "\n",
        " - To run or execute the code in a code cell, either click the small play icon  in the upper left of the cell or press `CTRL + ENTER` which the cell highlighted. Pressing `CTRL + ENTER` will work in either Google CoLab or Jupyter Notebook.\n",
        "\n",
        "<font color='green'>**Questions will be highlighted in Green. Try running the code cell just below to print 'HELLO WORLD'.**\n",
        "\n",
        "For each part there will be a guided portion introducing the material and some questions to work on individually and as part of your team. Many will include functions we have provided to you. If you have any questions or want to learn more about these, don't hesitate to ask one of our volunteers.\n",
        "\n",
        "We have also provided you with more background information than you necessarily need to complete the exercise.\n",
        "\n",
        " ## We indicated 'extra' sections in <font color=\"red\">**red**</font>, so feel free to **skip** these sections."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"HELLO WORLD\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx52Llu1TxDG",
        "outputId": "a985ac13-f7af-4655-8580-436175c0878c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HELLO WORLD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaaIqmt_ppnC"
      },
      "source": [
        "### **So What: Why Wildfires?**\n",
        "\n",
        "**Wildfires have a devastating impact on virtually every aspect of life**, ranging from their effects on public health, wildlife, societal well-being and economic stability. Overall, wildfire prediction plays a **vital role in disaster preparedness and response efforts**.\n",
        "\n",
        "<h2 align=\"right\"></h2> <img src=\"https://upload.wikimedia.org/wikipedia/commons/0/05/Burnout_ops_on_Mangum_Fire_McCall_Smokejumpers.jpg\" >\n",
        "\n",
        "\n",
        " - **Public Health & Safety**\n",
        "\n",
        "  -> Wildfire prediction allows rescue services to issue life saving early warnings and evacuation announcements to the public.\n",
        "\n",
        "  -> Wildfire prediction enables efficient allocation of resources by firefighters, where they can deploy equipment and aircraft appropriately according to the areas which are most likely to be affected.\n",
        "\n",
        " - **Environmental conservation**\n",
        "\n",
        "  -> Wildfire prediction means that we can minimise ecological disruptions for  endangered species by allowing for targeted conservation efforts and evacuation plans when fires approach their habitats.\n",
        "\n",
        "\n",
        " - **Property protection**\n",
        "\n",
        "  -> Wildfire prediction enables one to make proactive measures to protect their property if at risk by installing fire-resistant materials, having firefighting equipment on hand, and preparing for potential evacuations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWZ2Gxzu85-n"
      },
      "source": [
        "## **Part 0: Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEWLAT9aBxLP"
      },
      "source": [
        "### **Overview**\n",
        "\n",
        "Almost anywhere you look now, data is being used to drive decision making, invention, scientific innovation, and task automation.\n",
        "\n",
        "Some key challenges in almost all cases are:\n",
        " - **Matching what data you need with what questions you are trying to address.**\n",
        " - **Understanding and preparing your data so it can be useful in addressing your questions.**\n",
        "\n",
        "It is important to remember that each step of process is a decision point which may have an impact on your final conclusions.\n",
        "\n",
        "<h2 align=\"center\"></h2> <img src=\"https://imageio.forbes.com/blogs-images/gilpress/files/2016/03/Time-1200x511.jpg?format=jpg&width=960\">\n",
        "\n",
        "For more information, you can read the full [Forbes article](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mde2QirrG8yb"
      },
      "source": [
        "### **Introduction to the Data**\n",
        "\n",
        "Matching what data you need with what questions you are trying to address."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-uaFYIstS75"
      },
      "source": [
        "#### **The Wildfire Dataset**\n",
        "We will be working with a large, open-access **dataset on over 1.8 million [Wildfires](https://www.kaggle.com/datasets/rtatman/188-million-us-wildfires)**\n",
        " which occured in the **United States from 1992 to 2015**.\n",
        "\n",
        " **This dataset includes lots of information such as**:\n",
        " - the official ID of the wildfire\n",
        " - the State the fire was in (i.e. California)\n",
        " - the location of the fire in latitude and longitude coordinates\n",
        " - the final fire size in acres\n",
        " - what caused the fire\n",
        " - when the fire was discovered and reported\n",
        " - when the fire was contained\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiqmEcPz2qvE"
      },
      "source": [
        "#### **The Weather Dataset**\n",
        "As it happens we will also be **using a\n",
        "[weather dataset](https://www.kaggle.com/datasets/leternnoz/188-million-us-wildfires-weather-data) which contains information on the weather conditions at the time of each of the Wildfires, including**:\n",
        "- temperature (Celcius)\n",
        "- precipitation (measured in mm)\n",
        "- the wind speed (measured in km/hr)\n",
        "\n",
        "The **temperature, precipitation and wind speed averages** are given for different time periods leading up to the start of the Wildfire:\n",
        "- at the estimateed **'start' time** (0 days) of the Wildfire\n",
        "- the average over the **two weeks** (10 day) before the Wildfire\n",
        "- the average over the **month** (30 days) before the Wildfire\n",
        "- the average over the **2 months** (60 days) before the Wildfire\n",
        "- the average over the **6 months** (180 days) before the Wildfire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbAEmxG9CAf3"
      },
      "source": [
        "#### <font color=\"green\">**Quick Exercise**\n",
        "\n",
        "These are great datasets that we can use to ask and analyse questions such as:\n",
        "<font color=\"green\">\n",
        "- **Have wildfires become more or less frequent over time?**\n",
        "- **What areas are the most and least fire-prone?**\n",
        "- **Given this information, can you predict the size of a wildfire?**\n",
        "- **What trends you expect between Weather dataset and Wildfires datase?**\n",
        "</font>\n",
        "\n",
        "\n",
        "Take a minute as a group and **pick one these questions** you collectively find the most interesting to discuss and investigate.\n",
        "- **For each piece of information** in the Wildfire dataset, **discuss how could this information be useful to answer your queston**.\n",
        "- **As a group think of two other pieces of information** that might be good to have to address your question - this can be anything!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "994JJgPD9DMN"
      },
      "source": [
        "### **Preparing the Data**\n",
        "\n",
        "Commonly the process of data preparation is known as **ETL or Extract, Transform, Load**. It is the necessary and usually the most time consuming part of working with data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mes6sbQW2m7K"
      },
      "source": [
        "#### <font color=\"red\">**Background: The ETL Pipeline - time well spent**</font>\n",
        "\n",
        " We will use the anology of 'fitting a square peg in a round hole' to help describe the steps:\n",
        "\n",
        "- **Extract**: Think of this step as locating and acqiring a square peg. Where identify the data at its source location and acquire it locally.\n",
        "\n",
        "- **Transform**: Think of this step as taking our square peg, and engineering it to be the round peg shape with which we would like to work. We take our source data and prepare it for our needs. The data may be in the format you want already, but typically it is not quite formatted how you need it. The data may also contain extra information you are not conserned about. This typically involves cleaning, reformating, and augmenting the original data. This could be tasks like:\n",
        "  - removing or dealing with duplicate data, missing data, incorrect data, etc.\n",
        "  - selecting a subset of the original data (some might not be useful for you)\n",
        "  - changing the data format (i.e. from text to float values)\n",
        "  - sorting the data (i.e. by year or alphabetically)\n",
        "  - joining diffent piece of data (i.e. combining the wildfire and weather datasets)\n",
        "  - aggregating the data (i.e. taking daily logs and averaging them by month)\n",
        "\n",
        "- **Load**: Think of this step as putting your round peg in its spot. We chose what format we want to work with or save our transformed data in. This might be simply loading your data into computer memory, or saving your data to disk. Some common file formats for this are as:\n",
        "  - Structured data: SQL, Panda DataFrame, CSV, Excel, etc\n",
        "  - Semi-structured data: JSON, XML, etc\n",
        "  - Unstructured data: text, audio files, social media, satellite imagery, etc\n",
        "\n",
        "\n",
        "<h4 align=\"center\"></h4> <img src=\"https://preview.redd.it/which-tools-helps-you-make-such-animated-gif-for-data-v0-x1x1abmtu1ta1.gif?format=png8&s=4797fd55f2279881857be8e846488e689294c1dd\">\n",
        "\n",
        "**Additional resources**:\n",
        "\n",
        "[Wikipedia](https://en.wikipedia.org/wiki/Extract,_transform,_load)\n",
        "\n",
        "[Amazon Web Services](https://aws.amazon.com/what-is/etl/#:~:text=Extract%2C%20transform%2C%20and%20load%20(,and%20machine%20learning%20(ML).)\n",
        "\n",
        "<font color=\"red\">++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-UaJ1Hp9US7"
      },
      "source": [
        "#### **Extracting the Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Instructions**\n",
        "**Run the code cell below.** It contains a function to download and extract the data provided along with details on the process. Click on the cell if you would like to see the function and details.\n",
        "If you have questions about the function, what different parts do, etc, feel free to ask any of the volunteers."
      ],
      "metadata": {
        "id": "sQalYGjDt3xP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er98j_VpFPY3"
      },
      "source": [
        " <font color=\"red\">**Details**\n",
        "\n",
        "The function below preforms the **Extract** step of the ETL workflow. Both of our datasets are [Kaggle](https://www.kaggle.com/) open source datasets.\n",
        "\n",
        "To access Kaggle datasets through their public [API](https://en.wikipedia.org/wiki/API)  interface, you need a login ID and access token. We have taken care of that for you today. If you want to access these or other datasets in the future you will need to [register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2F) on your own with Kaggle.\n",
        "\n",
        "We use the standard python library `os` to work with the Google CoLab filesystem and Kaggle API to download the `.zip` dataset file, extract the contents and temporarily save them in Colab workspace.\n",
        "<font color=\"red\">++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmWhBpuNYyAM"
      },
      "outputs": [],
      "source": [
        "import os # library used to setup and access filesystem\n",
        "\n",
        "def download_and_extract_datasets(download=True, extract=True):\n",
        "  \"\"\"\n",
        "  Set up API token for kaggle dataset\n",
        "  \"\"\"\n",
        "\n",
        "  if download:\n",
        "    # token - this may need to be regenerated and copied again\n",
        "    kaggletoken='{\"username\":\"ej321icl\",\"key\":\"db927129b5c57b49a56553e9ada92494\"}'\n",
        "\n",
        "    # the token needs to be copied to /root/.kaggle/kaggle.json\n",
        "    path = \"/root/.kaggle\"\n",
        "    file = 'kaggle.json'\n",
        "\n",
        "    # use os library to set up path correctly\n",
        "    if not os.path.exists(path):\n",
        "      os.mkdir(path)\n",
        "\n",
        "    # copy to kaggle.json\n",
        "    with open(f'{path}/{file}','w') as f:\n",
        "      f.write(f'{kaggletoken}')\n",
        "\n",
        "    # set permission on the API token\n",
        "    !chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "    # download dataset from kaggle\n",
        "    !kaggle datasets download -d rtatman/188-million-us-wildfires\n",
        "    !kaggle datasets download -d leternnoz/188-million-us-wildfires-weather-data\n",
        "\n",
        "  if extract:\n",
        "    # unzip file\n",
        "    !unzip 188-million-us-wildfires.zip\n",
        "    !unzip 188-million-us-wildfires-weather-data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IlGlYmT9bVq"
      },
      "source": [
        "#### **Transforming the Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Instructions**\n",
        " **Run the code cell below.** It contains a function to pre-process the data that was downloaded and prepare it for future exercises, along with details on the pre-processing steps. Click on the cell if you would like to see the function and details.\n",
        "\n",
        " If you have questions about the function, what different parts do, etc, or are interested in the ways we fill in the data, feel free to ask any of the volunteers."
      ],
      "metadata": {
        "id": "PBlFKVVgui6W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSiyNbEsITkE"
      },
      "source": [
        "\n",
        "<font color=\"red\">**Details**\n",
        "\n",
        "The function below performs the **Transform** step of the ETL workflow. It does the following:\n",
        "\n",
        "- We start by reformating our input dataset into Pandas `DataFrames` - we will have more on Pandas shortly!\n",
        "\n",
        "- In our data query of the Weather dataset, we noticed a few duplicate entries. So we clean the data by filtering out the duplicate entries.\n",
        "\n",
        "-  We have decided we would like to combine the Wildfires and Weather datasets together into one larger dataset. We can do this by matching the weather conditions to the wildfire data via the unique ID of the wildfire.\n",
        "\n",
        "- We also noticed that there are many pieces of information not necessarily relevant to the questions we are investigating but are relevant to the US Government Agencies who originally supplied the data. This includes information such as which Agency reported the fire, the internal report number of the fire, etc. We will exclude this information from our final transformed dataset.\n",
        "\n",
        "- Finally, we also have identified some wildfires which have missing or incomplete data. Given our dataset contains over 1.8 million unique wildfires, we chose to exclude the ones with missing data from our transformed dataset. If we were dealing with a much smaller initial dataset, say 100 wildfires, a Data Scientist would have to think about how to fill in the missing data.\n",
        "\n",
        "<font color=\"red\">+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94DwB99K9N5Y"
      },
      "outputs": [],
      "source": [
        "import sqlite3  # library for manipulating SQL databases with Python\n",
        "import pandas as pd # library for formatting data as Excel like table with powerful data manipulation tools\n",
        "\n",
        "def clean_and_combine_datasets():\n",
        "  \"\"\"\n",
        "  Read the SQL and csv files into pandas dataframes and combine the two datasets based on the fire IDs\n",
        "  Also, clean the data by removing duplicates and dropping unneeded columns\n",
        "  \"\"\"\n",
        "  # load the data to pandas dataframes\n",
        "  fires = pd.read_sql(\"\"\"SELECT * FROM fires\"\"\", con=sqlite3.connect('./FPA_FOD_20170508.sqlite'))\n",
        "  fires_weather = pd.read_csv(\"./US_wildfire_weather_data.cvs\")\n",
        "\n",
        "  # remove duplicates from the weather dataset\n",
        "  print(f\"Number of removed duplicate rows in weather: {fires_weather.duplicated().sum()}\")\n",
        "  fires_weather = fires_weather.drop_duplicates()\n",
        "\n",
        "  # combine the datasets by matching on the unique fire ID\n",
        "  total_fire_data= fires_weather.set_index(\"OBJECTID\").join(fires.set_index(\"OBJECTID\"),on='OBJECTID')\n",
        "\n",
        "  # Drop data columns we will not be using\n",
        "  total_fire_data =total_fire_data.drop(['SOURCE_SYSTEM_TYPE',\n",
        "                                       'Shape','FIRE_CODE','SOURCE_SYSTEM','OWNER_DESCR','FPA_ID','FOD_ID'\n",
        "                                       ,'SOURCE_REPORTING_UNIT','NWCG_REPORTING_UNIT_ID','OWNER_CODE',\n",
        "                                       'CONT_TIME','NWCG_REPORTING_UNIT_NAME','SOURCE_REPORTING_UNIT_NAME','CONT_DOY',\n",
        "                                       'CONT_DATE','DISCOVERY_TIME','LOCAL_INCIDENT_ID','FIPS_CODE',\n",
        "                                       'COMPLEX_NAME', 'MTBS_FIRE_NAME','MTBS_ID','ICS_209_NAME','ICS_209_INCIDENT_NUMBER',\n",
        "                                      'LOCAL_FIRE_REPORT_ID','FIRE_NAME'], axis=1)\n",
        "\n",
        "  # Drop fires which have missing data - final data will be less than 1.88 million but more complete\n",
        "  fire_data_noMVs =total_fire_data.dropna(subset=['temp_mean_0', 'wspd_mean_0','temp_mean_10', 'wspd_mean_10','temp_mean_30', 'wspd_mean_20','temp_mean_60', 'wspd_mean_60','temp_mean_180', 'wspd_mean_180']) # drop all missing records\n",
        "\n",
        "  return fire_data_noMVs\n",
        "  # fire_data_noMVs.to_csv(filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdmN77dP9xDF"
      },
      "source": [
        "#### **Loading the Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5G1wZu8Iuwo"
      },
      "source": [
        "##### **Instructions**\n",
        "**Run the cell below. Note that this could take a couple of minutes.** If you have questions about the function, what different parts do, etc, feel free to ask any of the volunteers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font color=\"red\">**Details**</font>\n",
        "\n",
        "The code cell below preforms the **Load** step of the ETL workflow.\n",
        " It uses the two functions we have defined above for **extracting** and **transforming** the original datasets and sets a variable `fire_weather` to point at our final Pandas `DataFrame` object in computer memory. The variable `fire_weather` is now the dataset we will be working with for the remainder of the exercise.\n",
        "\n",
        "<font color=\"red\">+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</font>"
      ],
      "metadata": {
        "id": "Z7bctmfr4bEM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpJ6_JSVtvDs"
      },
      "outputs": [],
      "source": [
        "# Extact the datasets from Kaggle\n",
        "download_and_extract_datasets(download=True, extract=True)\n",
        "\n",
        "# Transforms the datasets\n",
        "fires_weather = clean_and_combine_datasets()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Ms6SWC-JHI"
      },
      "source": [
        "## **Part 1: Data Exploration: Getting to Know the Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-1Q7fy7_HCL"
      },
      "source": [
        "### **Overview: Importance of EDA**\n",
        "\n",
        "**EDA or Exploratory Data Analysis**, sometimes known as 'data wrangling', is a the main step in **connecting your data to questions it may be useful in addressing.**\n",
        "\n",
        "**Key skills in both data prepartation and data analysis** include being able to meaningfully:\n",
        "- query the data (Data Exploration)\n",
        "- visualise the data (Data Visualisation)\n",
        "- analyse the data\n",
        "- utilise the data (ML Applications)\n",
        "\n",
        "**In this section, we will introduce some of the most common Python methods and functions used for data manipulation** with some of the most useful functionality in each to investigate our Wildfires Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBDU8mOwe8A5"
      },
      "source": [
        "### **Introduction to Pandas**\n",
        "\n",
        "**Pandas is a very popular Python library for working with datasets.** It is an object oriented library built around organizing data similarly to what you find in Excel, with data arranged in rows and columns of a table object called a `DataFrame`. The library contains many methods and functions for manipulaitng the data.\n",
        "\n",
        "In the following section we will be covering:\n",
        "- `head` and `tail` : for quick querying of a dataset\n",
        "- `info` and `describe`: for summarising a dataset\n",
        "- various methods for filtering a dataset\n",
        "- `sort_values`, `groupby` and other static metrics: for looking at data in groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghxCIVdLERgU"
      },
      "source": [
        "#### **Quick Look: Head and Tail**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font color=\"red\">**Details**</font>\n",
        "\n",
        "These functions return a few rows of the dataset. They are very useful for quickly peeking at what information is in your dataset without accessing the all entries, and is especially convienient when the dataset is very large.\n",
        "[**Head**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) accesses the first few rows and [**Tail**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.tail.html) the last few rows of the dataset.\n",
        "\n",
        "For each function we can chose the number of rows to view by setting the value of the input arguement `n`. The default for both `head` and `tail` is 5 rows.\n",
        "\n",
        "<font color=\"red\">++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</font>"
      ],
      "metadata": {
        "id": "w-CLf-PyF3uT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ngoOdBlO1VD"
      },
      "source": [
        "##### **Instructions**\n",
        "**Run the following cells and take a look at the output.**\n",
        "  - try changing the value of `n`. Some values to test might be 0,1, -1, 100, 1000.\n",
        "  - for each value you try, what happened?\n",
        "  - for larger values of n, at what point are you seeing too much information?\n",
        "\n",
        "**Discuss as a group the following questions**:\n",
        "<font color=\"green\">\n",
        "  - **How many columns do we have in our dataset? Can we see them all by using `head` and `tail`?**\n",
        "  - **How many different data types (i.e. integer, float, text, boolean) can you identify in our dataset from just viewing these few rows?**\n",
        "  - **When might `tail` be more useful to run then `head`? Think beyond our current Wildfires dataset here. What if the data was ordered in some way?**\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlpkXhVsEopj"
      },
      "outputs": [],
      "source": [
        "# View the first 5 rows (by default) in our dataset\n",
        "fires_weather.head(n = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oci5JnNDFzOY"
      },
      "outputs": [],
      "source": [
        "# View the last 2 rows in our dataset\n",
        "fires_weather.tail(n=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmBrjW2HF_T8"
      },
      "source": [
        "#### **Dataset Summary: Info and Describe**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkdFYFrLPOxT"
      },
      "source": [
        "<font color=\"red\">**Details**</font>\n",
        "\n",
        "These functions return a summary of the entire Pandas `DataFrame`. They are very useful seeing a high-level summary of what is in your dataset.\n",
        "[**Info**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html) provides a concise summary of the data in the rows and columns while [**Describe**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) provides some summary statistics of the data, such as central tendency, dispersion and shape metrics.\n",
        "\n",
        "For `describe` there are some useful arguments we can set to make the output statistics more useful, depending on what we are querying the database for.\n",
        "\n",
        "- We can view different distributions by setting `percentile` ranges. The defaults are 25%, 50% and 75%, but there is nothing to stop us from asking for 10%, 20%, etc. We pass this argument as a Python `list` object. For example `percentiles = [.10, .20, .30, ,40., .50 , .60, .70, .80., .90]`\n",
        "- Some statistics are data type specific. The mean value of an `object` or `category` is not very descriptive nor is the frequency for `float` data. We can choose which [data types](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes) to `include` and `exclude` from the `describe` summary. These areguments can also take a Python `list` object. To `include` all datatypes we can either pass the arguement as `include = 'all'` or `include = [object,'category', 'number','datetime', 'timedelta', 'datetimez']. We can choose to `exclude` datatypes in a similar way. To exclude all text or string data types we could pass the arguement as `exclude =[object]`.   \n",
        "\n",
        "<font color=\"red\">++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MkDHQ8UPXT7"
      },
      "source": [
        "##### **Instructions**\n",
        "**Run the following cells and look at the output**.\n",
        "  - How many data types do we have?\n",
        "  - Are we missing any of the data types?\n",
        "  - Are there the same number of 'Non-Null' values for all our columns?\n",
        "  - What statistics do we see for numeric vs object data?\n",
        "  - Are there any columns where the statistics are not helpful?\n",
        "\n",
        "**Discuss as a group the following questions:**\n",
        "<font color=\"green\">\n",
        "  - **What do you find useful about running the `info` and `describe` functions**\n",
        "  - **What other summary information could be used to address our chosen question?** (If you think of something here, ask one of the volunteers and they may be able to show you another function!)\n",
        "  </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6VNrwXoI0jn"
      },
      "outputs": [],
      "source": [
        "# View a concise summary of our dataset rows and columns\n",
        "fires_weather.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGhoE9FgJyfQ"
      },
      "outputs": [],
      "source": [
        "# View summary statistics for our dataset - all the data\n",
        "fires_weather.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqhZe1vtQXcW"
      },
      "outputs": [],
      "source": [
        "# View summary statistics for our dataset - just the object data\n",
        "fires_weather.describe(include=[object]) ## object data refers to non-numeric fields for example strings, characters or words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BmRSEFuQe54"
      },
      "outputs": [],
      "source": [
        "# View summary statistics for our dataset - just the numeric data\n",
        "fires_weather.describe(include=['number'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLcvLDhpERlm"
      },
      "source": [
        "#### **Selecting Part of the Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06zfwak1Ph0Z"
      },
      "source": [
        "<font color=\"red\">**Details**</font>\n",
        "\n",
        "These functions allow you to select and work with just part of the entire Pandas `DataFrame`. They are very useful for querying the data to address more specific question or for working with a just a part of a very large dataset. It can also be useful to combine subseting the dataset with functions such as `head`, `tail`, `info` and `describe`. One can either just pass the new, smaller `DataFrame` to the screen or another function or can define a new variable to hold the object.\n",
        "- We can select specific data types to include in our subset with [`select_dtypes`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes). It has the `include` and `exclude` arguements that work just as they did for the `describe` function. And example of only subsetting the data to only include numeric data would be `select_dtypes(include=['number'])`\n",
        "- We can chose specific columns to include just by specifying their names in a Python `list` object. An example would be for our wildfires dataset, if we only want to see the discovery date and cause of the fire, we could specify only those columns as `fires_weather[['DISCOVERY_DATE', 'STAT_CAUSE_CODE']]`. This is a form of slicing our data and if you know about this already or want more infomation, feel free to ask our volunteers.\n",
        "- We can chose a portion of our data by including a query based on one or more of the columns. as `fires_weather[fires_weather['STAT_CAUSE_CODE']=='Lightning']`\n",
        "<font color=\"red\">++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKzE5oNoPoY8"
      },
      "source": [
        "##### **Instructions**\n",
        "**Run the following cells and take a look at the output**.\n",
        "  - Try including and excluding different columns based on datatype? Is this useful for our dataset?\n",
        "  - We learned how to select only a few `columns` by passing a list to our Pandas `DataFrame`. Can you figure out how to select only a few rows in a similar way?  \n",
        "\n",
        "\n",
        "**Discuss as a group the following questions:**\n",
        "<font color=\"green\">\n",
        "  - **If you could make a new `DataFrame` which only included columns useful for addressing your selected question, which would they be?**\n",
        "  - **For our dataset, is selecting by dtype or by a list of columns likely to be more useful? Why?**\n",
        "  </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1HKGG0YcKyn"
      },
      "outputs": [],
      "source": [
        "# select just the text columns in our dataset and only display the first 10 rows\n",
        "fires_weather.select_dtypes(include=[\"object\"]).head(n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncXIkrpVclfR"
      },
      "outputs": [],
      "source": [
        " # Only select the discovery date and cause of the fire\n",
        "fires_weather[['DISCOVERY_DATE', 'STAT_CAUSE_DESCR']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ViGJueed4sE"
      },
      "outputs": [],
      "source": [
        "# Only select the discovery date and cause of the fire and store in a new variable 'fires_origin'\n",
        "fires_origin = fires_weather[['DISCOVERY_DATE', 'STAT_CAUSE_DESCR']]\n",
        "\n",
        "# View the last 5 entries of the subset dataset\n",
        "fires_origin.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ESuvlY8jInI"
      },
      "outputs": [],
      "source": [
        "# Only select the state value for each fire and store it in a new variable 'fires_us_state'\n",
        "fires_us_state= fires_weather['STATE']\n",
        "\n",
        "# Sort the states from A-Z\n",
        "fires_us_state.sort_values(ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows where the fire was caused by lightning and store it in a new\n",
        "# variable 'fires_lightning'\n",
        "fires_lightning = fires_weather[fires_weather['STAT_CAUSE_DESCR']=='Lightning']\n",
        "\n",
        "# Subset the lightning only data to only the date of discovery and the fire cause\n",
        "# to check the filter worked.\n",
        "fires_lightning[['DISCOVERY_DATE', 'STAT_CAUSE_DESCR']]"
      ],
      "metadata": {
        "id": "35jsv8MIi1l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcRM5NBV5Toj"
      },
      "source": [
        "#### **Information on a Single Column**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao_J0G61QH1u"
      },
      "source": [
        "<font color=\"red\">**Details**</font>\n",
        "\n",
        "These functions are similar to the summary statistics for a Pandas `DataFrame` but are specific to a single column.\n",
        "\n",
        "Functions which are most useful on text, or categorical data versus continuous data, include [`value_counts`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.value_counts.html) and [`unique`](https://pandas.pydata.org/docs/reference/api/pandas.Series.unique.html). After selecting a single column, we can use `unique` to return all the unique, values in the column. Similarly, we can use `value_counts` to report the frequency by total count of items in our column.\n",
        "\n",
        "<font color=\"red\">+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkVkE1O4QRdN"
      },
      "source": [
        "##### **Instructions**\n",
        "\n",
        "**Run the following cells and take a look at the output**.\n",
        " - For the columns you are most interested in, look at the `unique` values and `value counts`. Is there anything surprising or want to know more about after doing this?\n",
        " - what do the fire size groups mean?\n",
        "\n",
        "**Discuss as a group the following questions:**\n",
        "<font color=\"green\">\n",
        "- **Share what you each found interesting or surprising when running `unique` and `value counts` for different columns.**\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjIcWSd57bhE"
      },
      "outputs": [],
      "source": [
        "# the unique US States in the dataset\n",
        "fires_weather['STATE'].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s42d1ilH7cG8"
      },
      "outputs": [],
      "source": [
        "# the unique fire size classes in the dataset\n",
        "fires_weather['FIRE_SIZE_CLASS'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6iKUF8OClyE"
      },
      "source": [
        "#### **Sort_values, Groupby, and group statistics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewDmbvy7QaM_"
      },
      "source": [
        "**Details**\n",
        "\n",
        "These functions allow you to sort and begin to aggregate Pandas `DataFrame` objects based on the specific content. They are very useful for querying the data to address more specific question which deal with groupings or ranking of the data. One can either just pass the new, organized DataFrame to the screen or another function or can define a new variable to hold the object.\n",
        "\n",
        "- We can use [`sort_values`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html) to specify one or more `columns` or `rows` to sort our Pandas `DataFrame`. Commonly one also passes a [`boolean`](https://en.wikipedia.org/wiki/Boolean_data_type) value to the `ascending` argument to declare how you want it to be sorted. This is done similarly by passing the name of a column or Python `list` of column names, for example `sort_values(by=['STATE'])` or `sort_values[by=['COUNTY', 'FIPS_NAME'], ascending=True)`\n",
        "\n",
        "- We can use [`groupby`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) to group the dataset by one or more `columns` or `rows`. This can be done as with `sort_values` by passing the name of a column or Python `list` of column names, for example `groupby(by=['STATE'])` or `groupby(by=['FIRE_YEAR', 'STATE'], ascending=True)`\n",
        "\n",
        "- There are a variety of functions available in Pandas such as [`mean`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mean.html), [`sum`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html),[`size`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.size.html), [`max`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.max.html) and [`min`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.min.html), which can be applied to a single column or a grouped Pandas `DataFrame`object to provide summary statistics over multiple rows. An example might be looking' at the largest Wildfire recorded in each state by discovery year, `fires_weather[['FIRE_YEAR', 'STATE','FIRE_SIZE']].groupby(by=['FIRE_YEAR', 'STATE']).max()`. In this example we first subset the data only by those columns we are intersted, before grouping and looking at the maximum value. Similarly we could look at `sum()` instead of `max()` if we were interested in the number of Wildfires in each state per year.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgsLcyEGQfDz"
      },
      "source": [
        "##### **Instructions**\n",
        "\n",
        "**Run the following cells and take a look at the output.**\n",
        " - For any of the examples provided, try modifying the data query.\n",
        " - If you add a new variable, such as 'FIRE_SIZE_CLASS' into the `groupby`, what happens?\n",
        " - How many items can you include in `groupby`? Does it make sense to `groupby` more than 4 groups? Why or why not?\n",
        "\n",
        "**Discuss as a group the following questions:**\n",
        "\n",
        "\n",
        "- <font color=\"green\">**Based on your selected question, with the knowledge of the functions you have so far, can you write code to answer your question? And if not, what are you missing?**\n",
        "- **Instead of a table output as we have seen to this point, a map or graph could be very informative. Which of these examples would be better displayed on a plot? on a map? Why?**\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjeOs13B16dw"
      },
      "outputs": [],
      "source": [
        "# Select only the year, state, and fire size columns for all Wildfires and report the maximum value for fire size group by discovery year and then US state\n",
        "fires_weather[['FIRE_YEAR', 'STATE','FIRE_SIZE']].groupby(by=['FIRE_YEAR', 'STATE']).max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB8cJAR9l0Mw"
      },
      "outputs": [],
      "source": [
        "# What do the fire size classes mean? We look at this by grouping the fire size by fire size class and looking at the summary statistics\n",
        "fires_weather[[\"FIRE_SIZE\",\"FIRE_SIZE_CLASS\"]].groupby([\"FIRE_SIZE_CLASS\"]).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ-nwtaa_GDD"
      },
      "source": [
        "###  <font color=\"green\">**EXERCISE 1: Data Exploration: Putting it all together**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cgeQigoQtLU"
      },
      "source": [
        "#### **Instructions**\n",
        "\n",
        "**Work in your groups to use the functions and knowledge of the dataset we have learned so far to answer the following questions.** These are designed to challenge you in using a combination of the functions above. It is worth noting that Pandas has many more functions then the ones we have outlined above, so if anyone in your group knows of others or is keen to look up another way of addressing the question - please do and share it with the rest of your group!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkeIPlCakD5Q"
      },
      "source": [
        "#### 1.How many of the US States had any Wildfires recorded between 1992 and 2015?\n",
        "\n",
        "*hint: you have seen very similar example code already*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkGO2sEdkPvm"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE ##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffGus7nKTwhw"
      },
      "source": [
        "#### 2.What is the total acreage burnt in each state per year?\n",
        "\n",
        "*hint: `groupby` will be very helpful here*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xko5zSSdkcxo"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE ##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf4H_UoRT-SI"
      },
      "source": [
        "#### 3.(A) What are the top 5 states with most acres burnt between 1992 and 2015? (B) Over just the last 5 years, between 2010 and 2015? (C) Are they the same or different?\n",
        "*hint: when applying sort_values to dataFrama with multiple columns, you may need to use `by=''` to specify which columns to sort by*\n",
        "\n",
        "*hint: a function we have not covered `.loc` may be helpful for this question and worth looking up [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html), in particular the conditional returns*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOVIHXf0kfOl"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE - PART A ##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdPzi2GykiAb"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE - PART B ##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqTQZNBVmvON"
      },
      "source": [
        "####  <font color=\"red\">BONUS: Create a new field \"Label\" that is 0 for fires of class size \"A\", \"B\" or \"C\" and 1 for all other fire sizes.</font>\n",
        "\n",
        "*hint: there are a few ways to answer this question, and most involve more python or pandas knowledge beyond what we have provided you. This question is best solved in your group. A few potential resources we have not covered today are: [`apply`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html), [how to add a new column](https://www.geeksforgeeks.org/adding-new-column-to-existing-dataframe-in-pandas/), [if/else statements](https://www.w3schools.com/python/python_conditions.asp), and [lambda functions](https://www.w3schools.com/python/python_lambda.asp).*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9AE858-n6tq"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE ##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4XFNkj7ysEh"
      },
      "source": [
        "<font color=\"red\">++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2qlwTL2UpL8"
      },
      "source": [
        "## **Part 2. Visualisation: Looking for Trends**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5dXkDWVV68X"
      },
      "source": [
        "### **Overview**\n",
        "\n",
        "**Data visualisation is an integral part of EDA or Exploratory Data Analysis**, which is an integral step in the data analysis process. Visualisations help data scientists **identify data quality issues, outliers, and potential relationships between variables**.\n",
        "\n",
        "Data Scientists, Researchers and PhD students do not work in isolation. **We are contiunally working in teams and leveraging each others strengths.** As we identify interesting trends and patterns in our dataset, **visualisation plays a key role in our ability to communicate our observations with others**. 'A picture is worth a thousand words, and for us, a plot, graph, map or video is just as powerful.\n",
        "\n",
        "**We will use the 'detective glasses' of visualisations to look at the dataset like our Wildfires dataset**, to look beyond the numbers, to pictures on a map to see where the wildfires happened.\n",
        "-  We look for **visual trends in our dataset in space** such as in the area, region and State where they occured.\n",
        "-  We also look for **trends over time, like watching a time-lapse video.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Lvw1squDnX"
      },
      "source": [
        "\n",
        "**In the follow section, you will be using pre-defined functions to visualise, and look for trends, in the Wildfires dataset**. These will help us find interesting things about the wildfires and how they behave.\n",
        "\n",
        "\n",
        "We focus our visual investigation first towards the same questions as you looked at in **Exercise 1 Q3**:\n",
        "- (A) **What are the top 5 states with most acres burnt between 1992 and 2015?**\n",
        "- (B) **What are the top 5 states with most acres burnt between 2010 and 2015?**\n",
        "- (C) **Are they the same or different?**\n",
        "\n",
        "You may or may not reinforce the observations you made in the previous section. You may also have some new insights into other trends in the data. Keep a keen eye out for trends as you explore the dataset, and don't hesitate to test your ideas by varying the values in the functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqlqxJJAUpL8"
      },
      "source": [
        "#### **Introduction to Data Visualisation Libraries**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO0voWp8szYN"
      },
      "source": [
        "<font color=\"red\">**Details**</font>\n",
        "\n",
        "We introduced Pandas in the last section as one of the most common Python libraries for manipulating large tablular datasets such as our combined Wildfires dataset. Here we use some of the most common Python libraries for data visualisation including:\n",
        "- [Matplotlib](https://matplotlib.org/) : useful for simple or quick visualisations. This library has been around for a long time so there are many easily searched solutions available.\n",
        "- [Bookeh](http://bokeh.org/) : a more powerful library for interactive and advanced visualisations. Similar to another common one called [Seaborn](https://seaborn.pydata.org/).\n",
        "- [Folium](https://medium.com/datasciencearth/map-visualization-with-folium-d1403771717): a more domain specific library for visualising data on a world map with options for interactive controls and time sequencing.\n",
        "\n",
        "<font color=\"red\">++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_jiJeL5UpL9"
      },
      "source": [
        "##### **Instructions**\n",
        "\n",
        "**Run the following cell to import the libraries we will use in the next section** If you have any questions, feel free to ask any of the volunteers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOv2Zv9YUpL9"
      },
      "outputs": [],
      "source": [
        "# we load the specific Matplotlib libraries and functions we know we need\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# we load the specific Bokeh libraries and functios we know we need\n",
        "from bokeh.plotting import figure, show, output_notebook\n",
        "from bokeh.models import ColumnDataSource, LogColorMapper, ColorBar, Title\n",
        "from bokeh.io.export import get_screenshot_as_png\n",
        "from bokeh.transform import linear_cmap\n",
        "from bokeh.palettes import Viridis256,Turbo256,Inferno256  # Import the correct palette\n",
        "\n",
        "# we load the folium libraries to plot it on a world map\n",
        "import folium\n",
        "from folium import IFrame\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "from PIL import Image\n",
        "import io\n",
        "import time\n",
        "import colorcet as cc\n",
        "import numpy as np\n",
        "\n",
        "# !pip install selenium - if creating a png doesn't work either you need selenium or we need to find another method that works on Chrome as well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyqL2vjjrLLI"
      },
      "source": [
        "### **EXERCISE 1: Wildfires Mapped: Quantity and Size**\n",
        "\n",
        "**In this section we will look at how we can understand the distribution of wildfires by using a map**. We can 'pin' where each fire occured relative to others based on its geographic reference coordinates (lat, long). For each fire\n",
        " 'pin' we can visualise different aspects of the wildfire data to see if there are visual trends. Here we focus on two metrics that we can plot for each location on the map:\n",
        "- The number of fires in a certain location over the last *n* years\n",
        "\n",
        "- The average size of the fires in that location over the last *n* years\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olX5juWarTxL"
      },
      "source": [
        "##### **Instructions**\n",
        "**Run the cells below.** These will create a function for pre-processing the dataset for visualisation and a function to plot maps. Click on the cell below if you would like to see the detail. If you have questions about the functions, what different parts do, etc, feel free to ask any of the volunteers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlfuE6kZuyq9"
      },
      "source": [
        "**Details**\n",
        "\n",
        "This function preprocesses the dataframe and extracts fire count or fire occurrence information between two years, denoted as 'year 1' and 'year 2' within the contiguous lower 48 US State (excluding Alaska, Hawaii, and Puerto Rico)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NuDXsrlUpL-"
      },
      "outputs": [],
      "source": [
        "def _pre_processing(df, year1, year2, option):\n",
        "    '''\n",
        "    This function preprocesses the dataframe and extracts fire count or fire\n",
        "    occurrence information between two years, denoted as 'year 1' and 'year 2'\n",
        "    within the contiguous lower 48 US State (excluding Alaska,\n",
        "    Hawaii, and Puerto Rico)\n",
        "\n",
        "    input argument:\n",
        "    df: Input dataframe containing wildfire information.\n",
        "    year 1: Lower limit of the time period (inclusive).\n",
        "    year 2: Upper limit of the time period (inclusive).\n",
        "    option: To obtain a dataframe that counts fire occurrences\n",
        "           within the specified time period,\n",
        "    select option = 'count'. To obtain a dataframe that returns\n",
        "           the mean fire size, select option = 'mean'.\n",
        "\n",
        "    output argument:\n",
        "    return the pre-processed dataframe\n",
        "    '''\n",
        "\n",
        "    # exclude both Alaska (AK), Hawaii (HI), and Puerto Rico (PR) from the dataset\n",
        "    df = df.loc[(df.loc[:,'STATE']!='AK') # exclude Alaska\n",
        "      & (df.loc[:,'STATE']!='HI') # exclude Hawaii\n",
        "      & (df.loc[:,'STATE']!='PR')] # exclude Puerto Rico\n",
        "\n",
        "    # crop the data within the min and max range of years\n",
        "    df = df[(df['FIRE_YEAR'] >= year1) & (df['FIRE_YEAR'] <= year2)]\n",
        "\n",
        "    # create a copy of the dataframe\n",
        "    new = df.copy()\n",
        "\n",
        "    # cluster wildfires by rounding the location to the nearest even\n",
        "    # latitude, longitude coordinates\n",
        "    new.loc[:,'LATITUDE'] = ((new.loc[:,'LATITUDE']*10).apply(np.floor))/10\n",
        "    new.loc[:,'LONGITUDE'] = ((new.loc[:,'LONGITUDE']*10).apply(np.floor))/10\n",
        "\n",
        "    # create a new label for the clustered location\n",
        "    new.loc[:,'LL_COMBO'] = new.loc[:,'LATITUDE'].map(str) + '-' + new.loc[:,'LONGITUDE'].map(str)\n",
        "\n",
        "    # create a new dataframe using groupby\n",
        "    grouped = new.groupby(['LL_COMBO', 'LATITUDE', 'LONGITUDE','FIRE_YEAR'])\n",
        "\n",
        "    # calculate the number of wildfires in the grouping\n",
        "    number_of_wf = grouped['FIRE_SIZE'].agg(['count']).reset_index()\n",
        "    # calculate the mean size of the wildfires in the grouping\n",
        "    size_of_wf = grouped['FIRE_SIZE'].agg(['mean']).reset_index()\n",
        "\n",
        "    # if the opiton 'count' is specified the number of wildfires is returned\n",
        "    if option == 'count':\n",
        "        return number_of_wf\n",
        "    # if the option 'size' is specified the avg size of wildfires is returned\n",
        "    elif option == 'mean':\n",
        "        return size_of_wf\n",
        "\n",
        "    # if something goes wrong, we return None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nq7pbc5-Bid"
      },
      "source": [
        "**Details**\n",
        "\n",
        "The combination of the functions defined for you below are called through the `plot_fire` function. They create 2 visualisations:\n",
        "- A map of the US displaying the occurances of wildfires over the specified time interval of years. The lighter the color of an area, the more wildfires have occured there.\n",
        "- A map of the US showing the size of the wildfires over the specified time interval in years. The lighter the color the more overlap between different wildfires.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_fire_size(df, year1, year2, option):\n",
        "\n",
        "    # get the fires by number and average size\n",
        "    df_processed = _pre_processing(df, year1, year2, option)\n",
        "\n",
        "    # use bokeh to build our map\n",
        "    source = ColumnDataSource(df_processed)\n",
        "\n",
        "    # set up the figure with title and plotting size\n",
        "    figtitle=f'{option.capitalize()} of wildfires (size) from {str(year1)} to {str(year2)}'\n",
        "    # p1 = figure(title=figtitle, toolbar_location=None, plot_width=600, plot_height=400)\n",
        "    p1 = figure(title=figtitle, toolbar_location=None, width=800, height=400)\n",
        "\n",
        "    # setup figure options\n",
        "    p1.title.text_font_size = '20px'\n",
        "    p1.title.align = 'center'\n",
        "    p1.background_fill_color = \"black\"\n",
        "    p1.grid.grid_line_color = None\n",
        "    p1.axis.visible = False\n",
        "    color_mapper = LogColorMapper(palette=cc.fire)\n",
        "\n",
        "    # Creating  and add a color bar  to the right side\n",
        "    p1.add_layout(ColorBar(color_mapper = color_mapper, label_standoff = 14, title = f'size: {option}'), 'right')\n",
        "    # add extra text describing the color scale in more detail\n",
        "    p1.add_layout(Title(text=\"Note: brighter color means more or larger wildfires\", align=\"center\"), \"below\")\n",
        "\n",
        "    # add fire locations to the map\n",
        "    glyph = p1.circle('LONGITUDE', 'LATITUDE', source=source,\n",
        "              color={'field': option, 'transform' : color_mapper},\n",
        "              size=1)\n",
        "    output_notebook()\n",
        "    show(p1)"
      ],
      "metadata": {
        "id": "XOfnI47Wm7eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd_FSP1L6ufY"
      },
      "source": [
        "##### **1.Compare fire quantity and size**\n",
        "**Run the code** in the follwing cells and analyse at the output. **Note that it may take one to two minutes**\n",
        "\n",
        "**Discuss as a group the following questions**:\n",
        "<font color=\"green\">\n",
        "- **2.1.1 Display on the map the occurrence and fire size distribution of wildfires using the data from 1995 to 2015 and then 2010 to 2015. Do you see any differences?** *(This question should be familiar from the previous part!)*\n",
        "- **2.1.2 Comparing the areas with the most occurances of wildfires to the areas with the largest wildfires. Do they overlap? What can you say about the fires in these areas?**\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## EDIT HERE ##########################\n",
        "# Enter here the years period you would like to investigate:\n",
        "year1 =\n",
        "year2 =\n",
        "#######################################\n",
        "\n",
        "# Filter the dataset back to only the relavent columns\n",
        "fires_size = fires_weather[['LATITUDE','LONGITUDE','FIRE_SIZE','FIRE_YEAR','STAT_CAUSE_DESCR','STATE','temp_mean_60']]\n",
        "\n",
        "# Create a spatial map based on the number of wildfires at an average location\n",
        "plot_fire_size(fires_size, year1, year2, 'count')\n",
        "\n",
        "# Create a spatial map based on the average size of wildfires at an average location\n",
        "plot_fire_size(fires_size, year1, year2, 'mean')"
      ],
      "metadata": {
        "id": "pJFWpf_doRFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha-DeZxX6Q5p"
      },
      "source": [
        "### **EXERCISE 2: Wildfires Through Time**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZZNW76dboKk"
      },
      "source": [
        "In this section, we are focusing on **wildfire distribution trends over time.** Please feel free to adjust the time period you wish to investigate. We will also explore how weather conditions have evolved during this time period and whether the changes in weather conditions may be linked to the changes in fire size and occurrence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH6BHV3_Bp53"
      },
      "source": [
        "\n",
        "The cells below contain two functions that generate different ways of visualising the trends in our data across time. These are:\n",
        "\n",
        "- **Visualise the correlation between weather and wildfires.** If we want to visualise how the wildfires varies over time and investigate whether there is any correlation between weather and wildfires, we need to plot the weather variation over time and fire occurance over time in the same plot. Function `plot_weather_correlation` will do that.\n",
        "\n",
        "- **Visualise the years as a movie.** Another way to examine time series data is by merging image frames into an animation. In this exercise, you'll have the opportunity to visualise the dynamic distribution of wildfires occurrences and observe it across various timeframes. The function `run_animate` to generate the animation from image frames at different years.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Instructions**\n",
        "\n",
        "**Run the following cell to create functions for visualisation** If you have any questions, feel free to ask any of the volunteers."
      ],
      "metadata": {
        "id": "WoURIpJly74-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_fires_correlation(df,year1,year2,column_header):\n",
        "    \"\"\"\n",
        "    Create two line plots of wildfire data (occurance and average size)\n",
        "    from the specified year range vs a secondary attribute in the dataset\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): The target data - assumes \"FIRE_YEAR\" is a valid column\n",
        "        year1 (int): The start year of the analysis.\n",
        "        year2 (int): The end year of the analysis.\n",
        "\n",
        "    Returns:\n",
        "        a matplotlib plot\n",
        "    \"\"\"\n",
        "    # Create time series of the chosen attribute\n",
        "\n",
        "    # subset the dataset to just the year and the chosen attribute\n",
        "    fire_temp = df[['FIRE_YEAR', column_header]]\n",
        "    # only select the rows between our start and end year, inclusive\n",
        "    # we need to reset the index when we do this\n",
        "    fire_temp = fire_temp[(fire_temp['FIRE_YEAR'] >= year1) & (fire_temp['FIRE_YEAR'] <= year2)].reset_index(drop=True)\n",
        "    # use groupby to get the mean of the chosen attribute per year\n",
        "    fire_temp = fire_temp.groupby(['FIRE_YEAR']).mean()\n",
        "\n",
        "    # Create time series of the number of fires per year\n",
        "    count_sum = []\n",
        "    for year, item in _pre_processing(df, year1, year2, 'count').groupby('FIRE_YEAR'):\n",
        "        count_sum.append(item['count'].sum())\n",
        "\n",
        "    # Create time series of the number of fires per year\n",
        "    size_avg = []\n",
        "    for year, item in _pre_processing(df, year1, year2, 'mean').groupby('FIRE_YEAR'):\n",
        "        size_avg.append(item['mean'].sum())\n",
        "\n",
        "    # Plot trends vs time using matplotlib\n",
        "\n",
        "    # Create figure with two subplots\n",
        "    fig, ax = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\n",
        "\n",
        "    # Plot the chosen attribute on the primary axis for both plots\n",
        "    for i in range(2):\n",
        "        ax[i].plot(fire_temp.index,fire_temp[column_header], 'tab:blue', label=f'{column_header.capitalize()}')\n",
        "        ax[i].set_xlabel('years')\n",
        "        ax[i].set_ylabel(f'{column_header.capitalize()}')\n",
        "        ax[i].set_xticks(range(year1, year2+1, 2))\n",
        "        ax[i].legend(loc='upper right')\n",
        "    # Create a second axis for the plot of fire occurance\n",
        "    ax1 = ax[0].twinx()\n",
        "    ax1.plot([int(x) for x in range(year1, year2+1)], count_sum, 'tab:orange', label='Fire occurance')\n",
        "    ax1.set_ylabel('Fire Count')\n",
        "    ax1.legend(loc='upper left')\n",
        "\n",
        "    # Create a second axis for the plot of fireavg size\n",
        "    ax2 = ax[1].twinx()\n",
        "    ax2.plot([int(x) for x in range(year1, year2+1)], size_avg, 'tab:orange', label='Fire size (avg)')\n",
        "    ax2.set_ylabel('Avg Size')\n",
        "    ax2.legend(loc='upper left')\n"
      ],
      "metadata": {
        "id": "Tlabtg8zwYR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGEN9j9iboKl"
      },
      "source": [
        "**Run the cell below.** If you have questions about the function, what different parts do, etc, feel free to ask any of the volunteers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXVkB3bYboKl"
      },
      "outputs": [],
      "source": [
        "def run_animate(df, year1, year2):\n",
        "    \"\"\"\n",
        "    Create an animated heatmap of wildfire data from the specified year range.\n",
        "\n",
        "    Parameters:\n",
        "        year1 (int): The start year of the analysis.\n",
        "        year2 (int): The end year of the analysis.\n",
        "\n",
        "    Returns:\n",
        "        folium.Map: A Folium map with the animated heatmap.\n",
        "    \"\"\"\n",
        "    # Create an empty list to store DataFrames for each year\n",
        "    df_years = []\n",
        "    # Create an empty list to store latitude and longitude data for each year\n",
        "    graph_data = []\n",
        "\n",
        "    # Loop through the specified range of years\n",
        "    for year in range(year1, year2+1):\n",
        "        # Filter the DataFrame to select data for the current year\n",
        "        df_years.append(df[df['FIRE_YEAR'] == year])\n",
        "        # Extract latitude and longitude values and convert to a list\n",
        "        graph_data.append(df_years[-1][['LATITUDE', 'LONGITUDE']].values.tolist())\n",
        "\n",
        "    # Create a Folium map centered at [40, -110] with a zoom level of 4\n",
        "    rain_map = folium.Map(location=[40, -110],\n",
        "                          zoom_start=4,\n",
        "                          min_zoom=3,\n",
        "                          width=1200,\n",
        "                          height=800,\n",
        "                          tiles='OpenStreetMap')\n",
        "\n",
        "    # Create a HeatMapWithTime layer using the latitude and longitude data\n",
        "    folium.plugins.HeatMapWithTime(\n",
        "        graph_data,\n",
        "        index=[_ for _ in range(year1, year2+1)],  # Time index for each year\n",
        "        radius=10,  # Radius of the heatmap points\n",
        "        gradient={  # Color gradient for the heatmap\n",
        "            .01: 'green',\n",
        "            .2: 'white',\n",
        "            .65: 'yellow',\n",
        "            1: 'red'\n",
        "        },\n",
        "        auto_play=True,  # Automatically play the animation\n",
        "    ).add_to(rain_map)\n",
        "\n",
        "    # Return the Folium map with the animated heatmap\n",
        "    return rain_map\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RFTvdmoBp54"
      },
      "source": [
        "##### 2.Plot trends over time\n",
        "\n",
        "**Run the code in the following cells and analyse at the output.**\n",
        "  - look at both the weather correlation plot and the animation for trends.\n",
        "  - try changing the value of range of years and make observations on the trends\n",
        "  - try changing the weather attribute and make observations on the trends.\n",
        "\n",
        "\n",
        "**Discuss as a group the following questions**:\n",
        "<font color=\"green\">\n",
        "- **2.2.1. Briefly describe the trend of wildfires over the past 20 years and discuss potential factors that might be causing this trend as viewed together on the Scatter Plot Map. Does this highlight any trends different from those in Exercise 2.1 above? What other data might be good to visualize this way?**\n",
        "- **2.2.2. Briefly describe the trend of wildfires over the past 20 years and discuss potential factors that might be causing this trend.**\n",
        "- **2.2.3. Look at the plots above and consider whether there is any correlation between fire occurance and temperature/wind speed in US.**</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaQjyJ7YboKl"
      },
      "source": [
        "###### 2.1 Plot the weather correlation graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## EDIT HERE ##########################\n",
        "# Enter here the years period you would like to investigate:\n",
        "year1 =\n",
        "year2 =\n",
        "#######################################\n",
        "\n",
        "# Choose the weather information you want to investigate, such as:\n",
        "# 'temp_mean_60' average temperature over the past 60 days\n",
        "#'wspd_mean_60 average wind speed over the past 60 days\n",
        "plot_fires_correlation(fires_weather,year1,year2, 'temp_mean_60')"
      ],
      "metadata": {
        "id": "uo4MRPr1z2CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgnxhSBbboKl"
      },
      "source": [
        "###### 2.2 Run the wildfire timelapse movie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1o-ulwPboKl"
      },
      "outputs": [],
      "source": [
        "## EDIT HERE ##########################\n",
        "# Enter here the years period you would like to investigate:\n",
        "year1 =\n",
        "year2 =\n",
        "#######################################\n",
        "\n",
        "# Run this cell to activate the animation\n",
        "rain_map = run_animate(fires_weather, year1, year2)\n",
        "rain_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkGfDqx44NGf"
      },
      "source": [
        "###  <font color=\"red\">**BONUS EXERCISE 3: Causes of Wildfires**</font>\n",
        "\n",
        "We look now at a different question of interest: **What are the causes of fire in different states of America?**\n",
        "\n",
        "In the wildfires dataset, there are 13 different causes of fire stated across the different states: `Structure`,`Firework`,`Powerline`,`Railroad`,`Smoking`,`Children`,`Campfire`,`Equipment Use`,`Missing/Undefined`,`Lightning`,`Arson`,`Miscellaneous`,`Debris Burning`\n",
        "\n",
        "In this exercise we will\n",
        "- review the distribution of all these different causes of wildfires using a bar chart\n",
        "- look at the distribution of causes on a map. Since there are a lot of different classes, we can group them together into 4 classes. The 4 classes are: natural, accidental, malicious and other.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8oW9Xs61Rxt"
      },
      "source": [
        "##### **Instructions**\n",
        "\n",
        "**Run the cell below.** The function below will create a new column in the data \"CAUSES_LABEL\" that will identify each wildfire as a larger class type.  If you have questions about the function, what different parts do, etc, feel free to ask any of the volunteers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZQiHBbpUpME"
      },
      "outputs": [],
      "source": [
        "def set_label(cat):\n",
        "    cause = 0\n",
        "    natural = ['Lightning']\n",
        "    accidental = ['Structure','Fireworks','Powerline','Railroad','Smoking','Children','Campfire','Equipment Use','Debris Burning']\n",
        "    malicious = ['Arson']\n",
        "    other = ['Missing/Undefined','Miscellaneous']\n",
        "    if cat in natural:\n",
        "        cause = 1\n",
        "    elif cat in accidental:\n",
        "        cause = 2\n",
        "    elif cat in malicious:\n",
        "        cause = 3\n",
        "    else:\n",
        "        cause = 4\n",
        "    return cause\n",
        "\n",
        "df_encode = fires_weather.copy()\n",
        "df_encode['CAUSES_LABEL'] = df_encode['STAT_CAUSE_DESCR'].apply(lambda x: set_label(x)) # I created a copy of the original df earlier in the kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAOHJEZ11SAq"
      },
      "source": [
        "\n",
        "**Run the cell below.** |The function below will plot a bar chart of different wildfire causes. If you have questions about the function, what different parts do, etc, feel free to ask any of the volunteers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1fjrOa3UpME"
      },
      "outputs": [],
      "source": [
        "def plot_cause_of_fire(df):\n",
        "    samples = np.arange(0, len(df), 15)\n",
        "    df_samlped = df_encode.iloc[samples]\n",
        "    df_sampled_groups = [item for item in df_samlped.groupby('CAUSES_LABEL')]\n",
        "    label_to_color = {1:\"#2ca02c\", 2:\"#d62728\", 3:\"#ff7f0e\", 4:'blue'}\n",
        "    label_names = {1:'Natural', 2:'Accidental', 3:'Malicious', 4:'Other'}\n",
        "    colors = [label_to_color[l] for l in df_encode['CAUSES_LABEL'].values[samples]]\n",
        "    fig, ax = plt.subplots(1,1,figsize=(6, 3))\n",
        "    samples = np.arange(0, 246596, 15)\n",
        "\n",
        "    for n, df_g in reversed(df_sampled_groups):\n",
        "        im = ax.scatter(df_g['LONGITUDE'], df_g['LATITUDE'],\n",
        "                        c=label_to_color[n],\n",
        "                        linewidths=0.001,\n",
        "                        marker='.',\n",
        "                        alpha=0.8,\n",
        "                        label=label_names[n])\n",
        "    ax.legend(loc='lower right', fontsize=10)\n",
        "    ax.set_xlim(-128, -65)\n",
        "    ax.set_ylim(22, 52)\n",
        "    ax.set_xlabel('Longitude')\n",
        "    ax.set_ylabel('Latitude')\n",
        "    ax.set_title('US Wildfire Causes Distribution 1995-20')\n",
        "\n",
        "    bounds = np.arange(1, 6, 1)\n",
        "    return df_sampled_groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv5pgei9CgXA"
      },
      "source": [
        "##### **Visualising different causes of wildfires**\n",
        "\n",
        "**Run the code in the follwing cells and analyse at the output.**\n",
        "  - make observations about the categories relative to geographical features you know about - does the distribution of causes make sense? are ther surprises?\n",
        "\n",
        "**Discuss as a group the following questions**:\n",
        "<font color=\"green\">\n",
        "- **2.3.1. Of the 13 causes of wildfires in the States, what are the most common and least common? Just from this information, can you make any predictions about what the map distribution might look like?**\n",
        "- **2.3.2. Using filter `loc[(df.STATE == \"AK\")]`(example for State of Alaska)  can you display how the causes of fire change by different States?**\n",
        "- **2.3.2. Look at the figure and and looking at the distribution of fire causes, what can you tell?**\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U14mm3ZzL7EM"
      },
      "outputs": [],
      "source": [
        "fires_weather[['STAT_CAUSE_DESCR']].value_counts().plot(kind='barh',color='coral')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Question 2. YOUR CODE HERE ##\n"
      ],
      "metadata": {
        "id": "pkktu1bHWJeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHYlQeoIL_kq"
      },
      "outputs": [],
      "source": [
        "## Question 3\n",
        "# Call the function `plot_cause_of_fire' and write your code here:\n",
        "# YOUR CODE HERE ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kjqWcu8Owl5"
      },
      "source": [
        "## **Part 3. Machine Learning: Utilising the Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1aktkPkpM2l"
      },
      "source": [
        "### **Overview**\n",
        "\n",
        "In the bonus question of exercise 1 we created a label for each fire that is 0 for \"small\" fires and 1 for \"large\" fires based on FIRE_SIZE_CLASS. In this exercise we will run a Machine Learning Model to predict that label and be able to classify a fire as \"small\" or \"large\". This is an example of a type of thing we might want to predict with wild fires. If we can anticipate a large wildfire we can minimise the damage it will cause.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2E5N6vQLlDl"
      },
      "source": [
        "#### **Background**\n",
        "\n",
        "In a predictive model every column in the data is assigned a coefficient. By multiplying the value in each field by the corresponding coefficient we can get a prediction. Let's look at an example of linear regression to understand this better.\n",
        "\n",
        "Let's say we want to predict the size of the fire in acres (variable $y$) and we have two columns in our data:\n",
        "- average temperature last month (variable $x$)\n",
        "- day of the month (variable $z$)\n",
        "\n",
        "then linear regression model will look like:\n",
        "\n",
        "$y = a + bx + cz$\n",
        "\n",
        "where $a$, $b$ and $c$ are the coefficients assigned by the model.\n",
        "\n",
        "**Example**\n",
        "\n",
        "| Avg_Temperature (x) | Day(z) |Predition (y)|\n",
        "| --- | --- | --- |\n",
        "| 25 | 3 | 1\n",
        "\n",
        "So applying regression model would look like:\n",
        "$1 = y = a + b*25 + c*3$\n",
        "\n",
        "So in this simple example we predict large fire.\n",
        "\n",
        "\n",
        "\n",
        "**Machine Learning models** refer to any model where the best coefficients are found automatically by the computer using an algorithm. So linear regression is an example of a simple Machine Learning model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpLIv6oK31Eg"
      },
      "source": [
        "#### **Random Forest Model**\n",
        "Today we will use Random Forest Model to label fires as large or small. Random Forest Models are often used in classifying problems similar to this one. It is called a Random Forest because it is actually a collection of small Decision Trees and prediction is made by combining the predictions from the individual Decision Trees.\n",
        "\n",
        "#### Decision Trees\n",
        "\n",
        "In a Decision Tree a prediciton is made by answering a series of questions. Starting with the first question (usually at the top or the left), we follow a path through the tree depending on the information we have, so we can arrive to a prediction.\n",
        "\n",
        "Here is an example of a Decision Tree to decide whether to walk or take the bus to school:\n",
        "<h2 align=\"center\"></h2> <img src=\"https://cdn-dfnaj.nitrocdn.com/xxeFXDnBIOflfPsgwjDLywIQwPChAOzV/assets/images/optimized/rev-4a6533c/www.displayr.com/wp-content/uploads/2018/07/what-is-a-decision-tree.png\">\n",
        "\n",
        "The image is from [Displayr](https://www.displayr.com/what-is-a-decision-tree/)\n",
        "\n",
        "\n",
        "A **Random Forest model** is made up of many Decision Trees, each one splitting the data in a different way. Each Decision Tree may not have a very high accuracy, but by taking the majority of the predictions we increase the accuracy. The more trees we have, the higher accuracy we expect.\n",
        "\n",
        "An example of a Randon Forest model with $n$ number of decision trees that predicts whether an animal is a cat or a dog:\n",
        "\n",
        "<h2 align=\"center\"></h2> <img src=\"https://images.datacamp.com/image/upload/v1677239993/image5_c214968fd6.png\">\n",
        "\n",
        "The image is from [Datacamp](https://www.datacamp.com/tutorial/random-forests-classifier-python)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Instructions**\n",
        "\n",
        "**Run the cell below.** This cell will prepare the data to run Machine Learning model. For more details click on the cell to see the description of the steps and the code. If you have questions about the function, what different parts do, etc, feel free to ask any of the volunteers."
      ],
      "metadata": {
        "id": "Zd_nYDT0kCfg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j37Dv5-r4DYo"
      },
      "source": [
        "<font color=\"red\">**Data Processing: Prepare Data for Modelling**</font>\n",
        "\n",
        "\"Garbage in, garbage out\" is a key concept for a modelling exercise. The model is only as good as the data used to develop the model. Therefore, to make sure the model is reliable and robust it is important to understand what data you are using and if the data requires any clean up is required.\n",
        "\n",
        "So far we have checked how many fields in the data have missing values (\"null\") and which fields are string fields (\"objects\") and which are numeric fields.\n",
        "\n",
        "We will review the data for potential predictive trends, assess the fields to include in the model and which to remove and finally assess the model predictions.\n",
        "\n",
        "When preparing a real life data for modelling there are certain data processing steps that should be considered:\n",
        "\n",
        "  - Creating a target field. This means adding another column to the data that captures what we want to predict using the Machine Learning model\n",
        "  - Sampling. There are several reasons we might want to sample the data we have:\n",
        "    *   Data is too large. Big data can often have millions of rows and thousands of columns, which takes a long time to process. However, we may not need all the records. For a dataset with fewer columns, we may be able to capture all the interactions in the data using fewer rows.\n",
        "    *   Proportionate representation of the target. If the frequency of the target classes is very different, we may sample to get more equal representation of all target classes.\n",
        "    *   Splitting the data into train and test datasets. We need both datasets to be representative of the whole population, so accurate sampling should be carried out.\n",
        "  - Convert `object` (`string`) columns to numerical equivalents. Machine Learning models may not be able to deal with `object` input columns, so we have to represent each category as a number\n",
        "  - Removing non-predictive information from the dataset. This information could be the data that is only captured after the wildfires are contained, or information used for calculating target field.\n",
        "\n",
        "The `data_preparation` function below carries out these steps to prepare the data and creates a new field called `fires_weather_for_ML`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ywoWIlyqo2b"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder # module to run Machine Learning Model\n",
        "import seaborn as sns # for user friendly visualisation\n",
        "\n",
        "\n",
        "# # Data Processing\n",
        "def data_preparation(dataFrameName):\n",
        "  dataFrameName_output = dataFrameName # take a copy of the dataset\n",
        "\n",
        "  ## 1. Sample the dataset, so there is at most 10,000 fires of each class size\n",
        "  dataFrameName_output = dataFrameName_output.groupby('FIRE_SIZE_CLASS', group_keys=False).apply(lambda x: x.sample(min(15000, len(x))))\n",
        "  dataFrameName_output['FIRE_SIZE_CLASS'].value_counts().plot(kind='barh')  ## plot the frequency of the target\n",
        "\n",
        "  ## 2. Create the Label field in case it was not created before:\n",
        "  dataFrameName_output[\"Label\"] = dataFrameName_output.apply(lambda row: 0 if row.FIRE_SIZE<=99.9 else 1, axis=1)\n",
        "  dataFrameName_output.groupby(\"Label\")['FIRE_SIZE_CLASS'].value_counts().plot(kind='barh')  ## plot the frequency of the target\n",
        "  ## 3. Replace string fields with numeric values instead.\n",
        "  #     A lot of functions used for analysis work only with numeric fields:\n",
        "  object_fields = dataFrameName_output.select_dtypes(['object']).columns\n",
        "  le = LabelEncoder()\n",
        "  for i in object_fields:\n",
        "    dataFrameName_output[i] = le.fit_transform(dataFrameName_output[i])\n",
        "\n",
        "  # return the processed dataFrame\n",
        "  return dataFrameName_output\n",
        "\n",
        "fires_weather_test = fires_weather\n",
        "fires_weather_for_ML = data_preparation(fires_weather)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMUV92Zgntx4"
      },
      "source": [
        "<font color=\"red\">**Details**</font>\n",
        "\n",
        "From the plot above we can see that the 1.48 million fires were sampled down to 59,919 wildfires based on the FIRE_SIZE_CLASS. This type of sampling is called Stratified Random Sampling, where we pick a category and sample from each category.\n",
        "\n",
        "Additional questions:\n",
        "- Why do you think we chose Stratified Random Sampling?\n",
        "- What other types of sampling do you know?\n",
        "- Do you think they are a good way to sample in this case? Why or why not?\n",
        "\n",
        "\n",
        "<font color=\"red\">++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVCeVTMN9Ksl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import sklearn.ensemble as ske\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "def data_split(dataFrameName, drop_fields):\n",
        "\n",
        "  # 1. Drop non-predictive fields\n",
        "  dataFrameName_copy = dataFrameName\n",
        "  dataFrameName_copy = dataFrameName.drop(drop_fields, axis=1)\n",
        "\n",
        "  # 2. Prepare the target data\n",
        "  Y = np.array(dataFrameName_copy[\"Label\"])\n",
        "  le = LabelEncoder()\n",
        "  le.fit(Y)\n",
        "  Y_transofrmed = le.transform(Y)\n",
        "  Y_transofrmed = Y_transofrmed.reshape(-1,1)\n",
        "\n",
        "  # 3. Drop the target from data\n",
        "  dataFrameName_copy = dataFrameName_copy.drop('Label', axis=1)\n",
        "\n",
        "  # Split the data into predictive and target\n",
        "  X = torch.tensor(dataFrameName_copy.values, dtype=torch.float32)\n",
        "  y= torch.tensor(Y_transofrmed, dtype=torch.float32)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
        "\n",
        "  return X_train, X_test, y_train, y_test\n",
        "\n",
        "def RandomForestModel(Xtrain,ytrain, depth=None):\n",
        "\n",
        "  randomForest_model = ske.RandomForestClassifier(max_depth=None)\n",
        "  randomForest_model = randomForest_model.fit(Xtrain, np.ravel(ytrain))\n",
        "\n",
        "  return randomForest_model\n",
        "\n",
        "drop_fields = [\"NWCG_REPORTING_AGENCY\",\"FIRE_SIZE\", \"FIRE_SIZE_CLASS\"]\n",
        "## Run the functions to create the training and testing datatsets and run the model\n",
        "X_train, X_test, y_train, y_test = data_split(fires_weather_for_ML,drop_fields)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXERCISE 1: Run Random Forest Model**\n",
        "\n",
        "In this section we will train the model using Python module `sklearn`. The function `RandomForestModel` will train the model and save the coefficients for each column in the data so we can use the model on different datasets to predict the fire size.\n",
        "\n"
      ],
      "metadata": {
        "id": "MgUhPkOmqsob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Instructions**\n",
        "\n",
        "**Run the cell below**.\n",
        "\n",
        "**Discuss as a group the following questions**:\n",
        "\n",
        "<font color=\"green\">\n",
        "\n",
        "- **What does the accuracy mean?**\n",
        "- **How do you think the accuracy is calculated?**\n",
        "</font>\n",
        "\n",
        "*hint*: recall what the flag we are predicting looks like\n",
        "\n"
      ],
      "metadata": {
        "id": "wldg0ehJrIjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "ANSWER:\n",
        "The accuracy is the percentage of correct predictions. So all fires that were correctly labeled as 1 or 0 are flagged as 1 and fires where the prediction is different to the actual are flagged as 0. Then the mean of that flag is taken. For more details please refer to the confusion matrix in bonus question below\n"
      ],
      "metadata": {
        "id": "g8TpmY0ofGol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "randomForest_model = RandomForestModel(X_train,y_train)\n",
        "# Using the Random Forest model trained above, score out the test datasets and  calculate the accuracy.\n",
        "print(\"The model accuracy is \", randomForest_model.score(X_test,y_test)*100,\"%\")"
      ],
      "metadata": {
        "id": "1PXb2BdbrUuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2zsrQbo7IAg"
      },
      "source": [
        "### <font color=\"red\">**BONUS EXERCISE 2: Identifying Trends**</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyGT4IRK4YD9"
      },
      "source": [
        "#### **Identifying Trends**\n",
        "\n",
        "Looking for trends between the data columns and the target helps to see, which information might be useful and which is not.\n",
        "\n",
        "The following code plots the regression line between \"NWCG_REPORTING_AGENCY\" field (column 15 in the data `dataFrame`) and the target \"Label\" (y-vars).\n",
        "\n",
        " The downward slope of regression line means higher values of \"NWCG_REPORTING_AGENCY\" correlate with the lower values of \"Label\" (i.e. smaller fires).\n",
        "\n",
        "Even if we see a trend does not necessarily mean that we should include this column as predictor. Therefore we should ask the following questions:\n",
        "- Do we expect to see a correlation between the target and this field?\n",
        "- Does this trend make sense?\n",
        "- If not, why do we see this trend?\n",
        "- Is this information available at the point of origin of fire? When is it filled in?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NjYGBFy5TvX"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(fires_weather_for_ML, x_vars=fires_weather_for_ML.columns.drop('Label')[15], y_vars=['Label'], markers='',\n",
        "             plot_kws={'x_jitter': 0.1, 'y_jitter': 0.1, 'scatter_kws': {'alpha': 0.2}},\n",
        "             kind='reg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJO5qXV7qp6A"
      },
      "source": [
        "#### 2.1 Which columns would you not expect to be predictive?\n",
        "Run `info` function (from Exercise 1) to get the list of columns in the data.\n",
        "\n",
        "**Discuss as a group the following question:**\n",
        "\n",
        "<font color=\"green\"> **Which columns you think will and will not be useful to predict whether the wildfire will be large or small?**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Run info to get a list of columns\n",
        "## YOUR CODE HERE ##"
      ],
      "metadata": {
        "id": "Kr5EyXwRycfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgfXKLbvwOeL"
      },
      "source": [
        "#### 2.2 Run trend analysis for all the columns.\n",
        "\n",
        "**Change the code in the next cell** to run the trend analysis for several columns at once.\n",
        "\n",
        "*hint*: Recall from Exercise 1 the syntax [start:end] to slice data, this can also be used to select columns.\n",
        "\n",
        "**Discuss as a group the following question:**\n",
        "\n",
        "<font color=\"green\">\n",
        "\n",
        "- **What trends do you see, in particular with weather information?**\n",
        "- **What trends do you see for the fields you guessed in Exercise 2.1 as \"Not predictive\". Is this surprising?**\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--MN3s-1woTg"
      },
      "outputs": [],
      "source": [
        "# Change the code above to show trend for first 8 columns only\n",
        "##YOUR CODE HERE ##\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUJhVnO28DCu"
      },
      "outputs": [],
      "source": [
        "# Plot columns 8 to 15\n",
        "##YOUR CODE HERE ##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8CslZwu-dlF"
      },
      "outputs": [],
      "source": [
        "# Plot columns 16 to 24\n",
        "##YOUR CODE HERE ##\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru1Nonp08KPt"
      },
      "source": [
        "#### 3.3 Remove unpredictive fields.\n",
        "Based on your analysis in question 3.2 type below the fields that you choose to drop from the dataset to be used for modelling.\n",
        "\n",
        "HINT: Also don't forget to remove fields that were used to create the target \"Label\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE ##\n",
        "drop_fields = [\"OWNER_CODE\", #YOUR CODE HERE#\n",
        "               ]"
      ],
      "metadata": {
        "id": "DjdkND00gznT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"red\">**BONUS EXERCISE 3: Assessing Model Performance**</font>\n"
      ],
      "metadata": {
        "id": "Oac8S61P0ybs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5dY-NstCNaZ"
      },
      "source": [
        "#### **3.1 Checking Model Prediction**\n",
        "\n",
        "Most models are not always correct and will have some incorrect predictions. However, **some incorrect predictions are worst than others**. Predicting a small fire and being wrong can have much bigger damage than the other way round.\n",
        "\n",
        "So there are two types of errors:\n",
        "- **False Positive**: predicting a large fire (=1) that is small (=0)\n",
        "- **False Negative**: predicting a small fire (=0) that is large (1)\n",
        "\n",
        "**Confusion matrix** shows the number of correct classifications and the number of errors. So we can compare the number of False Positives and False Negatives.\n",
        "\n",
        "\n",
        "#### **Instructions**:\n",
        "**Run the cell below**. It will create a confusion matrix.\n",
        "\n",
        "**Discuss as a group the following questions:**\n",
        "\n",
        "<font color=\"green\">\n",
        "\n",
        "-**Which squares do you think show**:</font>\n",
        "\n",
        "- <font color=\"green\"> True Positive\n",
        "- <font color=\"green\"> True Negative\n",
        "- <font color=\"green\"> False Positive\n",
        "- <font color=\"green\"> False Negative\n",
        "\n",
        "-**Using the counts in the squares how do we calculate the accuracy percentage that we saw when we run model?**</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neMdIaDECuJD"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test,randomForest_model.predict(X_test))\n",
        "disp = ConfusionMatrixDisplay(cm,display_labels=randomForest_model.classes_)\n",
        "disp.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRFO0pfoKrxR"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP2_VPZuVZzK"
      },
      "source": [
        "# **Wrap Up**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Wildfire Activity Summary**\n",
        "\n",
        "**Congrats! You have now experienced a bit of life as a Data Scientist, or a day in the life of a PhD student.**\n",
        "\n",
        "The large US wildfire dataset we worked with today is used by Governments, Scientists, and Researchers around the world in shaping their models to better predict wildfire occurances. Better wildfire prediction aims at mitigating the human and environmental consequences of such destructive natural disasters.\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=https://i.cbc.ca/1.6879130.1686931407!/fileImage/httpImage/south-african-firefighters.jpg width=\"600\" align='center'/>\n",
        "</div>\n",
        "\n",
        "[*Extra: A good article on how tackling wildfires is an international effort*](https://www.cbc.ca/news/canada/edmonton/international-firefighters-canada-wildfires-1.6879084)\n",
        "\n",
        "\n",
        "**Through our Exploratory Data Analysis (EDA) of the data, we were able to make observations of some interesting trends and qualitative correlations.** Our observations, along with all of the data analysis tools you have seen today, form the basis for building ML models to help predict wildfires.\n",
        "\n",
        "**Hopefuly today has brought you:**\n",
        "- some fun with coding!\n",
        "- opportunity to meet and collaborate with other students\n",
        "- exposure to some new concepts and ideas\n",
        "- the chance to meet PhD students and ask them lots of questions"
      ],
      "metadata": {
        "id": "a6qvL_IvZsqS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaaClPyaYH2k"
      },
      "source": [
        "## **Presentation Phd Students**\n",
        "\n",
        "If you haven't guessed yet, all the volunteers here today use coding, maths, and analytical thinking to tackle their own big questions. We have asked each to briefly share with you a bit on their research and how they incorporate coding. Perhaps if we are lucky, they will also share a quick tip or piece of advice!\n",
        "\n",
        "\n",
        "(Quick sharing of phd students research work. ) 1/2min each"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkzOtoyeYObk"
      },
      "source": [
        "## **Quiz with prizes!**\n",
        "\n",
        "**LInk to Mentimeter:**\n",
        "\n",
        "https://www.menti.com/aldcwm2k1xnt\n",
        "\n",
        "or\n",
        "go to [menti.com](menti.com) and type in the code: 4769 5762\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1uVuac4YRmS"
      },
      "source": [
        "## **Q&A (mentimeter) and feedback**\n",
        "\n",
        "**Link to Feedback Mentimeter:**\n",
        "\n",
        "https://www.menti.com/alyrsxbnot95\n",
        "\n",
        "or go to [menti.com](menti.com) and type in the code: 5651 9695\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thank you for coming and we hope you enjoyed the day!"
      ],
      "metadata": {
        "id": "JKj48taKrdFE"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "sQalYGjDt3xP",
        "PBlFKVVgui6W",
        "K5G1wZu8Iuwo",
        "7_jiJeL5UpL9",
        "olX5juWarTxL",
        "WoURIpJly74-",
        "r8oW9Xs61Rxt"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}